{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyiwn\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SynsetRelations.MERO_MEMBER_COLLECTION',\n",
       " 'SynsetRelations.ABILITY_VERB',\n",
       " 'SynsetRelations.CAUSATIVE',\n",
       " 'SynsetRelations.CAPABILITY_VERB',\n",
       " 'SynsetRelations.MERO_COMPONENT_OBJECT',\n",
       " 'SynsetRelations.HOLO_PORTION_MASS',\n",
       " 'SynsetRelations.FUNCTION_VERB',\n",
       " 'SynsetRelations.HOLO_COMPONENT_OBJECT',\n",
       " 'SynsetRelations.HYPERNYMY',\n",
       " 'SynsetRelations.ENTAILMENT',\n",
       " 'SynsetRelations.ALSO_SEE',\n",
       " 'SynsetRelations.MERO_FEATURE_ACTIVITY',\n",
       " 'SynsetRelations.HOLO_PLACE_AREA',\n",
       " 'SynsetRelations.MODIFIES_VERB',\n",
       " 'SynsetRelations.ATTRIBUTES',\n",
       " 'SynsetRelations.MERO_PORTION_MASS',\n",
       " 'SynsetRelations.MODIFIES_NOUN',\n",
       " 'SynsetRelations.HOLO_FEATURE_ACTIVITY',\n",
       " 'SynsetRelations.MERO_STUFF_OBJECT',\n",
       " 'SynsetRelations.TROPONYMY',\n",
       " 'SynsetRelations.MERO_PLACE_AREA',\n",
       " 'SynsetRelations.HOLO_MEMBER_COLLECTION',\n",
       " 'SynsetRelations.HYPONYMY',\n",
       " 'SynsetRelations.SIMILAR',\n",
       " 'SynsetRelations.MERO_POSITION_AREA',\n",
       " 'SynsetRelations.HOLO_POSITION_AREA',\n",
       " 'SynsetRelations.HOLO_STUFF_OBJECT']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(str, pyiwn.SynsetRelations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-07:22:26:52,14 INFO     [iwn.py:43] Loading telugu language synsets...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "వెలివేయబడిన\tఇంకా జన్మించని వాడు.\n",
      "అశుభం\tమంచి లేదా మంగళకరం కానిది\n",
      "ప్రవేశంలేని\tలోనికిరాని\n",
      "పవిత్రస్థానం\tఆ స్థలం పవిత్రమైనదిగా భావిస్తారు\n",
      "శివాలయం\tశివుని ఆరాధించుటకు శివుని విగ్రహం ప్రతిష్టించిన మందిరం\n",
      "అపవిత్ర_స్థానం\tఏదైతే పవిత్ర స్థలం కాదో\n",
      "వచ్చిన\tఎవరైతే వస్తారో\n",
      "జన్మించిన\tతల్లి గర్భం నుండి కొత్తగా భూమిపైకి రావడం\n",
      "నీతికిసంబంధించిన_పని\tఅవినీతికి సంబంధించినది కాదు\n",
      "పండించిన\tఏదైన ఉత్పత్తి చేసిన.\n",
      "------------------------------------------------------------------------------------------\n",
      "word: వెలివేయబడిన\n",
      "similars: []\n",
      "hyponyms: []\n",
      "hypernym: []\n",
      "entailment: []\n",
      "troponymy: []\n",
      "------------------------------------------------------------------------------------------\n",
      "word: అశుభం\n",
      "similars: []\n",
      "hyponyms: []\n",
      "hypernym: []\n",
      "entailment: []\n",
      "troponymy: []\n",
      "------------------------------------------------------------------------------------------\n",
      "word: ప్రవేశంలేని\n",
      "similars: []\n",
      "hyponyms: []\n",
      "hypernym: []\n",
      "entailment: []\n",
      "troponymy: []\n",
      "------------------------------------------------------------------------------------------\n",
      "word: పవిత్రస్థానం\n",
      "similars: []\n",
      "hyponyms: [Synset('గుడి.noun.452'), Synset('పూజగది.noun.3616'), Synset('ప్రార్థనాస్థలం.noun.3619'), Synset('తీర్థ_స్థానము.noun.6007'), Synset('పీఠం.noun.6852')]\n",
      "hypernym: [Synset('స్థలం.noun.1973')]\n",
      "entailment: []\n",
      "troponymy: []\n",
      "------------------------------------------------------------------------------------------\n",
      "word: శివాలయం\n",
      "similars: []\n",
      "hyponyms: []\n",
      "hypernym: [Synset('దేవాలయం.noun.451')]\n",
      "entailment: []\n",
      "troponymy: []\n",
      "------------------------------------------------------------------------------------------\n",
      "word: అపవిత్ర_స్థానం\n",
      "similars: []\n",
      "hyponyms: []\n",
      "hypernym: [Synset('స్థలం.noun.1973')]\n",
      "entailment: []\n",
      "troponymy: []\n",
      "------------------------------------------------------------------------------------------\n",
      "word: వచ్చిన\n",
      "similars: []\n",
      "hyponyms: []\n",
      "hypernym: []\n",
      "entailment: []\n",
      "troponymy: []\n",
      "------------------------------------------------------------------------------------------\n",
      "word: జన్మించిన\n",
      "similars: []\n",
      "hyponyms: []\n",
      "hypernym: []\n",
      "entailment: []\n",
      "troponymy: []\n",
      "------------------------------------------------------------------------------------------\n",
      "word: నీతికిసంబంధించిన_పని\n",
      "similars: []\n",
      "hyponyms: [Synset('అహింస..noun.3275'), Synset('ప్రాయశ్చిత్తము.noun.9511'), Synset('లోకసేవ.noun.10002')]\n",
      "hypernym: [Synset('పని.noun.183')]\n",
      "entailment: []\n",
      "troponymy: []\n",
      "------------------------------------------------------------------------------------------\n",
      "word: పండించిన\n",
      "similars: []\n",
      "hyponyms: []\n",
      "hypernym: []\n",
      "entailment: []\n",
      "troponymy: []\n",
      "------------------------------------------------------------------------------------------\n",
      "word: చెడునడత\n",
      "similars: []\n",
      "hyponyms: [Synset('అధికారము.noun.8412')]\n",
      "hypernym: [Synset('దుర్గుణం.noun.2377')]\n",
      "entailment: []\n",
      "troponymy: []\n",
      "------------------------------------------------------------------------------------------\n",
      "word: గర్వం\n",
      "similars: []\n",
      "hyponyms: []\n",
      "hypernym: [Synset('దుర్గుణం.noun.2377')]\n",
      "entailment: []\n",
      "troponymy: []\n",
      "------------------------------------------------------------------------------------------\n",
      "word: అత్యాచారం\n",
      "similars: []\n",
      "hyponyms: []\n",
      "hypernym: [Synset('చెడ్డపని.noun.18')]\n",
      "entailment: []\n",
      "troponymy: []\n",
      "------------------------------------------------------------------------------------------\n",
      "word: అవకతవకలు\n",
      "similars: []\n",
      "hyponyms: []\n",
      "hypernym: [Synset('ఆపద.noun.5024')]\n",
      "entailment: []\n",
      "troponymy: []\n",
      "------------------------------------------------------------------------------------------\n",
      "word: విటమిన్లు\n",
      "similars: []\n",
      "hyponyms: []\n",
      "hypernym: [Synset('పోషించేతత్వం.noun.11769')]\n",
      "entailment: []\n",
      "troponymy: []\n",
      "------------------------------------------------------------------------------------------\n",
      "word: మంచినడవడిక\n",
      "similars: []\n",
      "hyponyms: []\n",
      "hypernym: [Synset('మంచిగుణం.noun.75')]\n",
      "entailment: []\n",
      "troponymy: []\n",
      "------------------------------------------------------------------------------------------\n",
      "word: ఇష్టానుసారం\n",
      "similars: []\n",
      "hyponyms: []\n",
      "hypernym: [Synset('పని.noun.183')]\n",
      "entailment: []\n",
      "troponymy: []\n",
      "------------------------------------------------------------------------------------------\n",
      "word: చెడ్డపని\n",
      "similars: []\n",
      "hyponyms: [Synset('అత్యాచారం.noun.13'), Synset('అపరాధం.noun.171'), Synset('దుష్టకార్యం.noun.2245'), Synset('అవినీతి.noun.2713'), Synset('పాపం.noun.2716')]\n",
      "hypernym: [Synset('పని.noun.183')]\n",
      "entailment: []\n",
      "troponymy: []\n",
      "------------------------------------------------------------------------------------------\n",
      "word: ధాన్యం\n",
      "similars: []\n",
      "hyponyms: [Synset('మొక్కజొన్న.noun.1394'), Synset('బజారు..noun.1467'), Synset('పప్పు_ధాన్యం.noun.1967'), Synset('పంట.noun.3833'), Synset('గోధుమలు.noun.4045')]\n",
      "hypernym: [Synset('ఆహారపదార్థం.noun.20')]\n",
      "entailment: []\n",
      "troponymy: []\n",
      "------------------------------------------------------------------------------------------\n",
      "word: ఆహారపదార్థం\n",
      "similars: []\n",
      "hyponyms: [Synset('ధాన్యం.noun.19'), Synset('కబాబ్.noun.410'), Synset('కోవా.noun.430'), Synset('అన్నసత్రం.noun.436'), Synset('గోధుమరవ్వ.noun.440')]\n",
      "hypernym: [Synset('పదార్ధం.noun.744')]\n",
      "entailment: []\n",
      "troponymy: []\n"
     ]
    }
   ],
   "source": [
    "iwn = pyiwn.IndoWordNet(lang=pyiwn.Language.TELUGU)\n",
    "synsets = iwn.all_synsets()\n",
    "pairs = []\n",
    "# Iterate through each synset\n",
    "for i, synset in enumerate(synsets[:10]):\n",
    "    pairs.append((synset.head_word(), synset.gloss()))\n",
    "    print(f\"{pairs[i][0]}\\t{pairs[i][1]}\")\n",
    "\n",
    "for i, synset in enumerate(synsets[:20]):\n",
    "    print(f\"-\"*90)\n",
    "    print(f\"word: {synset.head_word()}\")\n",
    "    similars = iwn.synset_relation(synset, pyiwn.SynsetRelations.SIMILAR)\n",
    "    print(f\"similars: {similars[:5]}\")\n",
    "\n",
    "    hyponyms = iwn.synset_relation(synset, pyiwn.SynsetRelations.HYPONYMY)\n",
    "    print(f\"hyponyms: {hyponyms[:5]}\")\n",
    "\n",
    "    hypernyms = iwn.synset_relation(synset, pyiwn.SynsetRelations.HYPERNYMY)\n",
    "    print(f\"hypernym: {hypernyms[:5]}\")\n",
    "\n",
    "    entailments = iwn.synset_relation(synset, pyiwn.SynsetRelations.ENTAILMENT)\n",
    "    print(f\"entailment: {entailments[:5]}\")\n",
    "\n",
    "    troponymys = iwn.synset_relation(synset, pyiwn.SynsetRelations.TROPONYMY)\n",
    "    print(f\"troponymy: {troponymys[:5]}\")\n",
    "\n",
    "    # hypernyms = iwn.synset_relation(synset, pyiwn.SynsetRelations.HYPERNYMY)\n",
    "    # print(f\"hypernym: {hypernyms[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-07:23:21:18,47 INFO     [iwn.py:43] Loading telugu language synsets...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSV file with word presence check created: telugu/WaW_te_lemma.tsv\n"
     ]
    }
   ],
   "source": [
    "import pyiwn\n",
    "import csv\n",
    "\n",
    "# Function to read the large corpus and create a set of words\n",
    "def load_corpus_as_set(corpus_file):\n",
    "    word_set = set()\n",
    "    with open(corpus_file, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            # Assuming each line contains space-separated words\n",
    "            words = line.strip().split()\n",
    "            word_set.update(words)\n",
    "    return word_set\n",
    "\n",
    "# Function to extract WordNet word-definition pairs\n",
    "def extract_word_definition_pairs():\n",
    "    synsets = iwn.all_synsets()\n",
    "    pairs = []\n",
    "    # Iterate through each synset\n",
    "    for synset in synsets:\n",
    "        pairs.append((synset.head_word(), synset.gloss(), synset))\n",
    "\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def create_tsv_with_presence_check(wordnet_data, corpus_file, output_file):\n",
    "    corpus_word_set = load_corpus_as_set(corpus_file)\n",
    "\n",
    "    word_def_pairs = extract_word_definition_pairs()\n",
    "\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as tsv_file:\n",
    "        tsv_writer = csv.writer(tsv_file, delimiter='\\t')\n",
    "        tsv_writer.writerow(['index', 'word_a', 'word_b', 'label'])\n",
    "\n",
    "        for index, (word, definition, synset) in enumerate(word_def_pairs):\n",
    "            is_present = 1 if word in corpus_word_set else 0\n",
    "\n",
    "            if is_present == 0 and \"_\" not in word and \" \" not in word:\n",
    "                similars = iwn.synset_relation(synset, pyiwn.SynsetRelations.SIMILAR)\n",
    "                hypernyms = iwn.synset_relation(synset, pyiwn.SynsetRelations.HYPERNYMY)\n",
    "                hyponyms = iwn.synset_relation(synset, pyiwn.SynsetRelations.HYPONYMY)\n",
    "                similar_words_list = []\n",
    "                for similar in similars:\n",
    "                    similar_words_list.append(similar.head_word())\n",
    "                for hypernym in hypernyms:\n",
    "                    similar_words_list.append(hypernym.head_word())\n",
    "                for hyponym in hyponyms:\n",
    "                    similar_words_list.append(hyponym.head_word())\n",
    "\n",
    "                if len(similar_words_list) > 0:\n",
    "                    tsv_writer.writerow([index, synset.head_word(), similar_words_list, 1])\n",
    "\n",
    "# Example usage\n",
    "LANG = \"te\"\n",
    "LANG_FULL = \"telugu\"\n",
    "corpus_file = f'/media/saketh/New Volume/NAACL 2025/Datasets/{LANG}/{LANG}_10M_splits.txt'  # Your 4GB large corpus file\n",
    "output_file = f'{LANG_FULL}/WaW_{LANG}_lemma.tsv'\n",
    "\n",
    "iwn = pyiwn.IndoWordNet(lang=pyiwn.Language.TELUGU)\n",
    "\n",
    "# Create the TSV file\n",
    "create_tsv_with_presence_check(extract_word_definition_pairs(), corpus_file, output_file)\n",
    "\n",
    "print(f\"TSV file with word presence check created: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Negative Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-25:21:22:21,786 INFO     [iwn.py:43] Loading telugu language synsets...\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "import pyiwn\n",
    "import csv\n",
    "\n",
    "# Function to read the large corpus and create a set of words\n",
    "def load_corpus_as_set(corpus_file):\n",
    "    word_set = set()\n",
    "    with open(corpus_file, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            # Assuming each line contains space-separated words\n",
    "            words = line.strip().split()\n",
    "            word_set.update(words)\n",
    "    return word_set\n",
    "\n",
    "# Function to extract WordNet word-definition pairs\n",
    "def extract_word_definition_pairs():\n",
    "    synsets = iwn.all_synsets()\n",
    "    pairs = []\n",
    "    # Iterate through each synset\n",
    "    for synset in synsets:\n",
    "        pairs.append((synset.head_word(), synset.gloss(), synset))\n",
    "\n",
    "    return pairs\n",
    "\n",
    "def generate_negative_samples(word_def_pairs, positive_pairs_file, output_file):\n",
    "    # Load existing positive pairs\n",
    "    positive_pairs = set()\n",
    "    word_pos_neighbors = defaultdict(set)\n",
    "    all_words = set()\n",
    "    \n",
    "    with open(positive_pairs_file, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f, delimiter='\\t')\n",
    "        next(reader)  # Skip header\n",
    "        for row in reader:\n",
    "            word_a = row[1]\n",
    "            similar_words = eval(row[2])  # Convert string representation of list back to list\n",
    "            \n",
    "            # Store positive pairs and build neighborhood dictionary\n",
    "            for word_b in similar_words:\n",
    "                positive_pairs.add((word_a, word_b))\n",
    "                positive_pairs.add((word_b, word_a))  # Add both directions\n",
    "                word_pos_neighbors[word_a].update(similar_words)\n",
    "                word_pos_neighbors[word_b].update([word_a])\n",
    "            \n",
    "            all_words.add(word_a)\n",
    "            all_words.update(similar_words)\n",
    "    \n",
    "    # Convert to list for random sampling\n",
    "    all_words = list(all_words)\n",
    "    \n",
    "    def is_valid_negative_pair(word1, word2):\n",
    "        # Check if this pair or its reverse is not already positive\n",
    "        if (word1, word2) in positive_pairs or (word2, word1) in positive_pairs:\n",
    "            return False\n",
    "            \n",
    "        # Check if words share any neighbors (indirect relationship)\n",
    "        if word_pos_neighbors[word1] & word_pos_neighbors[word2]:\n",
    "            return False\n",
    "            \n",
    "        # Additional WordNet-based checks\n",
    "        word1_synsets = iwn.synsets(word1)\n",
    "        word2_synsets = iwn.synsets(word2)\n",
    "        \n",
    "        for s1 in word1_synsets:\n",
    "            for s2 in word2_synsets:\n",
    "                # Check if they share any hypernym\n",
    "                hyper1 = set(iwn.synset_relation(s1, pyiwn.SynsetRelations.HYPERNYMY))\n",
    "                hyper2 = set(iwn.synset_relation(s2, pyiwn.SynsetRelations.HYPERNYMY))\n",
    "                if hyper1 & hyper2:\n",
    "                    return False\n",
    "                \n",
    "                # Check for other semantic relationships\n",
    "                if s2 in iwn.synset_relation(s1, pyiwn.SynsetRelations.ENTAILMENT):\n",
    "                    return False\n",
    "        \n",
    "        return True\n",
    "\n",
    "    # Generate negative samples\n",
    "    negative_pairs = set()\n",
    "    required_negative_samples = len(positive_pairs)\n",
    "    \n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f, delimiter='\\t')\n",
    "        writer.writerow(['index', 'word_a', 'word_b', 'label'])\n",
    "        \n",
    "        attempt_count = 0\n",
    "        max_attempts = required_negative_samples * 10  # Limit attempts to avoid infinite loop\n",
    "        \n",
    "        while len(negative_pairs) < required_negative_samples and attempt_count < max_attempts:\n",
    "            word1 = random.choice(all_words)\n",
    "            word2 = random.choice(all_words)\n",
    "            \n",
    "            if word1 != word2 and is_valid_negative_pair(word1, word2):\n",
    "                negative_pairs.add((word1, word2))\n",
    "                writer.writerow([len(negative_pairs), word1, word2, 0])\n",
    "            \n",
    "            attempt_count += 1\n",
    "            \n",
    "        if len(negative_pairs) < required_negative_samples:\n",
    "            print(f\"Warning: Could only generate {len(negative_pairs)} negative samples out of {required_negative_samples} requested\")\n",
    "        \n",
    "    return negative_pairs\n",
    "\n",
    "# Usage\n",
    "LANG = \"te\"\n",
    "LANG_FULL = \"telugu\"\n",
    "corpus_file = f'/media/saketh/New Volume/NAACL 2025/Datasets/{LANG}/{LANG}_10M_splits.txt'  # Your 4GB large corpus file\n",
    "output_file = f'{LANG_FULL}/WaW_{LANG}_copy.tsv'\n",
    "iwn = pyiwn.IndoWordNet(lang=pyiwn.Language.TELUGU)\n",
    "word_def_pairs = extract_word_definition_pairs()\n",
    "negative_samples_file = f'{LANG_FULL}/WaW_{LANG}_lemma_negative.tsv'\n",
    "negative_pairs = generate_negative_samples(word_def_pairs, output_file, negative_samples_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create balanced test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data Statistics:\n",
      "total_samples: 10000\n",
      "positive_samples: 5000\n",
      "negative_samples: 5000\n",
      "unique_words: 1610\n",
      "avg_word_length: 8.2207\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "def create_balanced_test_data(train_positive_file, train_negative_file, test_size, output_file):\n",
    "    \"\"\"\n",
    "    Creates a balanced test dataset with equal positive and negative samples,\n",
    "    ensuring no overlap with training data.\n",
    "    \"\"\"\n",
    "    # Load all training data to avoid test-train overlap\n",
    "    train_pairs = set()\n",
    "    train_words = set()\n",
    "    \n",
    "    # Load positive training pairs\n",
    "    with open(train_positive_file, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f, delimiter='\\t')\n",
    "        next(reader)  # Skip header\n",
    "        for row in reader:\n",
    "            word_a = row[1]\n",
    "            similar_words = eval(row[2])\n",
    "            for word_b in similar_words:\n",
    "                train_pairs.add((word_a, word_b))\n",
    "                train_pairs.add((word_b, word_a))\n",
    "                train_words.add(word_a)\n",
    "                train_words.add(word_b)\n",
    "\n",
    "    # Load negative training pairs\n",
    "    with open(train_negative_file, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f, delimiter='\\t')\n",
    "        next(reader)  # Skip header\n",
    "        for row in reader:\n",
    "            word_a, word_b = row[1], row[2]\n",
    "            train_pairs.add((word_a, word_b))\n",
    "            train_pairs.add((word_b, word_a))\n",
    "            train_words.add(word_a)\n",
    "            train_words.add(word_b)\n",
    "\n",
    "    # Get all synsets and their relations for positive sampling\n",
    "    all_synsets = iwn.all_synsets()\n",
    "    synset_relations = defaultdict(list)\n",
    "    \n",
    "    for synset in all_synsets:\n",
    "        head_word = synset.head_word()\n",
    "        if head_word not in train_words:  # Only consider words not in training\n",
    "            # Get related synsets\n",
    "            similars = iwn.synset_relation(synset, pyiwn.SynsetRelations.SIMILAR)\n",
    "            hypernyms = iwn.synset_relation(synset, pyiwn.SynsetRelations.HYPERNYMY)\n",
    "            hyponyms = iwn.synset_relation(synset, pyiwn.SynsetRelations.HYPONYMY)\n",
    "            \n",
    "            for related_synset in similars + hypernyms + hyponyms:\n",
    "                related_word = related_synset.head_word()\n",
    "                if related_word not in train_words:\n",
    "                    synset_relations[head_word].append(related_word)\n",
    "\n",
    "    def generate_positive_samples(needed_samples):\n",
    "        positive_pairs = []\n",
    "        available_words = list(synset_relations.keys())\n",
    "        \n",
    "        while len(positive_pairs) < needed_samples and available_words:\n",
    "            word_a = random.choice(available_words)\n",
    "            if synset_relations[word_a]:\n",
    "                word_b = random.choice(synset_relations[word_a])\n",
    "                if (word_a, word_b) not in train_pairs and (word_b, word_a) not in train_pairs:\n",
    "                    positive_pairs.append((word_a, word_b, 1))\n",
    "                    # Remove used words to ensure diverse sampling\n",
    "                    if len(synset_relations[word_a]) == 1:\n",
    "                        available_words.remove(word_a)\n",
    "        \n",
    "        return positive_pairs\n",
    "\n",
    "    def generate_negative_samples(needed_samples, positive_test_pairs):\n",
    "        negative_pairs = []\n",
    "        all_test_words = set([word for pair in positive_test_pairs for word in pair[:2]])\n",
    "        all_test_words = list(all_test_words)\n",
    "        \n",
    "        def is_valid_negative_pair(word1, word2):\n",
    "            # Check if not in training pairs\n",
    "            if (word1, word2) in train_pairs or (word2, word1) in train_pairs:\n",
    "                return False\n",
    "                \n",
    "            # Check if not in positive test pairs\n",
    "            if (word1, word2, 1) in positive_test_pairs or (word2, word1, 1) in positive_test_pairs:\n",
    "                return False\n",
    "                \n",
    "            # Check WordNet relationships\n",
    "            word1_synsets = iwn.synsets(word1)\n",
    "            word2_synsets = iwn.synsets(word2)\n",
    "            \n",
    "            for s1 in word1_synsets:\n",
    "                for s2 in word2_synsets:\n",
    "                    # Check various semantic relationships\n",
    "                    if (s2 in iwn.synset_relation(s1, pyiwn.SynsetRelations.SIMILAR) or\n",
    "                        s2 in iwn.synset_relation(s1, pyiwn.SynsetRelations.HYPERNYMY) or\n",
    "                        s2 in iwn.synset_relation(s1, pyiwn.SynsetRelations.HYPONYMY)):\n",
    "                        return False\n",
    "            \n",
    "            return True\n",
    "\n",
    "        attempts = 0\n",
    "        max_attempts = needed_samples * 10\n",
    "        \n",
    "        while len(negative_pairs) < needed_samples and attempts < max_attempts:\n",
    "            word1 = random.choice(all_test_words)\n",
    "            word2 = random.choice(all_test_words)\n",
    "            \n",
    "            if word1 != word2 and is_valid_negative_pair(word1, word2):\n",
    "                negative_pairs.append((word1, word2, 0))\n",
    "            \n",
    "            attempts += 1\n",
    "        \n",
    "        return negative_pairs\n",
    "\n",
    "    # Generate balanced test data\n",
    "    samples_per_class = test_size // 2\n",
    "    positive_samples = generate_positive_samples(samples_per_class)\n",
    "    negative_samples = generate_negative_samples(samples_per_class, positive_samples)\n",
    "    \n",
    "    # Combine and shuffle test data\n",
    "    test_data = positive_samples + negative_samples\n",
    "    random.shuffle(test_data)\n",
    "    \n",
    "    # Save test data\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f, delimiter='\\t')\n",
    "        writer.writerow(['index', 'word_a', 'word_b', 'label'])\n",
    "        for idx, (word_a, word_b, label) in enumerate(test_data):\n",
    "            writer.writerow([idx, word_a, word_b, label])\n",
    "    \n",
    "    # Generate statistics\n",
    "    stats = {\n",
    "        'total_samples': len(test_data),\n",
    "        'positive_samples': len(positive_samples),\n",
    "        'negative_samples': len(negative_samples),\n",
    "        'unique_words': len(set(word for pair in test_data for word in pair[:2])),\n",
    "        'avg_word_length': sum(len(word) for pair in test_data for word in pair[:2]) / (len(test_data) * 2)\n",
    "    }\n",
    "    \n",
    "    return test_data, stats\n",
    "\n",
    "# Usage example\n",
    "test_size = 10000  # Specify desired test set size (will be split equally between positive and negative)\n",
    "test_file = f'{LANG_FULL}/WaW_{LANG}_test.tsv'\n",
    "\n",
    "test_data, stats = create_balanced_test_data(\n",
    "    train_positive_file=f'{LANG_FULL}/WaW_{LANG}_positive.tsv',\n",
    "    train_negative_file=f'{LANG_FULL}/WaW_{LANG}_negative.tsv',\n",
    "    test_size=test_size,\n",
    "    output_file=test_file\n",
    ")\n",
    "\n",
    "print(\"Test Data Statistics:\")\n",
    "for key, value in stats.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
